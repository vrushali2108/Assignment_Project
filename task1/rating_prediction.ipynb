{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Yelp Review Rating Prediction via Prompting\n",
        "\n",
        "This notebook implements 3 different prompting approaches to classify Yelp reviews into 1-5 star ratings.\n",
        "\n",
        "## Dataset\n",
        "Using Yelp Reviews dataset from Kaggle: https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from typing import Dict, List, Tuple\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Gemini API\n",
        "api_key = os.getenv('GEMINI_API_KEY')\n",
        "if api_key:\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "else:\n",
        "    print(\"Warning: GEMINI_API_KEY not found. Please set it in .env file\")\n",
        "    model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset (adjust path as needed)\n",
        "# If dataset is large, sample 200 rows for evaluation\n",
        "try:\n",
        "    df = pd.read_csv('yelp_reviews.csv')\n",
        "    print(f\"Loaded {len(df)} reviews\")\n",
        "    \n",
        "    # Sample 200 rows if dataset is larger\n",
        "    if len(df) > 200:\n",
        "        df_sample = df.sample(n=200, random_state=42)\n",
        "        print(f\"Sampled {len(df_sample)} reviews for evaluation\")\n",
        "    else:\n",
        "        df_sample = df\n",
        "        \n",
        "    # Ensure we have required columns\n",
        "    if 'text' not in df_sample.columns:\n",
        "        # Try common column names\n",
        "        text_col = [c for c in df_sample.columns if 'text' in c.lower() or 'review' in c.lower()][0]\n",
        "        df_sample['text'] = df_sample[text_col]\n",
        "    \n",
        "    if 'stars' not in df_sample.columns:\n",
        "        # Try common column names\n",
        "        stars_col = [c for c in df_sample.columns if 'star' in c.lower() or 'rating' in c.lower()][0]\n",
        "        df_sample['stars'] = df_sample[stars_col]\n",
        "        \n",
        "    print(f\"\\nDataset columns: {df_sample.columns.tolist()}\")\n",
        "    print(f\"\\nStar distribution:\")\n",
        "    print(df_sample['stars'].value_counts().sort_index())\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"Dataset file not found. Please download from Kaggle and place as 'yelp_reviews.csv'\")\n",
        "    print(\"Creating sample data for demonstration...\")\n",
        "    # Create sample data for demonstration\n",
        "    sample_reviews = [\n",
        "        (\"This restaurant is absolutely amazing! The food was delicious and the service was outstanding. I will definitely come back.\", 5),\n",
        "        (\"Terrible experience. Food was cold, service was slow, and the place was dirty. Never coming back.\", 1),\n",
        "        (\"It was okay. Nothing special, but nothing terrible either. Average food and service.\", 3),\n",
        "        (\"Great food but the wait time was too long. Staff was friendly though.\", 4),\n",
        "        (\"The worst restaurant I've ever been to. Food poisoning and rude staff.\", 1),\n",
        "        (\"Excellent service and good food. A bit pricey but worth it.\", 4),\n",
        "        (\"Mediocre at best. Expected more for the price.\", 2),\n",
        "        (\"Perfect! Everything was amazing from start to finish.\", 5),\n",
        "    ]\n",
        "    \n",
        "    # Expand to 200 samples\n",
        "    expanded = []\n",
        "    for i in range(25):\n",
        "        for text, stars in sample_reviews:\n",
        "            expanded.append((f\"{text} (Sample {i+1})\", stars))\n",
        "    \n",
        "    df_sample = pd.DataFrame(expanded, columns=['text', 'stars'])\n",
        "    print(f\"Created {len(df_sample)} sample reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompting Approach 1: Direct Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt_approach_1(review_text: str) -> str:\n",
        "    \"\"\"Direct classification prompt - straightforward approach\"\"\"\n",
        "    prompt = f\"\"\"Classify the following Yelp review into a star rating from 1 to 5.\n",
        "\n",
        "Review: {review_text}\n",
        "\n",
        "Return your response as a JSON object with the following structure:\n",
        "{{\n",
        "    \"predicted_stars\": <number between 1 and 5>,\n",
        "    \"explanation\": \"<brief reasoning for the assigned rating>\"\n",
        "}}\n",
        "\n",
        "JSON Response:\"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompting Approach 2: Chain-of-Thought"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt_approach_2(review_text: str) -> str:\n",
        "    \"\"\"Chain-of-thought prompting - reasoning through the classification\"\"\"\n",
        "    prompt = f\"\"\"Analyze the following Yelp review step by step:\n",
        "\n",
        "Review: {review_text}\n",
        "\n",
        "Step 1: Identify positive aspects mentioned in the review\n",
        "Step 2: Identify negative aspects mentioned in the review\n",
        "Step 3: Assess the overall sentiment (very negative, negative, neutral, positive, very positive)\n",
        "Step 4: Map the sentiment to a star rating (1=very negative, 2=negative, 3=neutral, 4=positive, 5=very positive)\n",
        "Step 5: Consider the intensity and specificity of the feedback\n",
        "\n",
        "Based on your analysis, return a JSON object:\n",
        "{{\n",
        "    \"predicted_stars\": <number between 1 and 5>,\n",
        "    \"explanation\": \"<detailed reasoning based on your step-by-step analysis>\"\n",
        "}}\n",
        "\n",
        "JSON Response:\"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompting Approach 3: Few-Shot Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prompt_approach_3(review_text: str) -> str:\n",
        "    \"\"\"Few-shot learning prompt with examples\"\"\"\n",
        "    prompt = f\"\"\"You are classifying Yelp reviews into star ratings (1-5). Here are some examples:\n",
        "\n",
        "Example 1:\n",
        "Review: \"This restaurant is absolutely amazing! The food was delicious and the service was outstanding.\"\n",
        "Response: {{\"predicted_stars\": 5, \"explanation\": \"Highly positive language with strong praise for both food and service\"}}\n",
        "\n",
        "Example 2:\n",
        "Review: \"Terrible experience. Food was cold, service was slow, and the place was dirty.\"\n",
        "Response: {{\"predicted_stars\": 1, \"explanation\": \"Multiple serious complaints about food quality, service, and cleanliness\"}}\n",
        "\n",
        "Example 3:\n",
        "Review: \"It was okay. Nothing special, but nothing terrible either. Average food and service.\"\n",
        "Response: {{\"predicted_stars\": 3, \"explanation\": \"Neutral sentiment with average ratings for both food and service\"}}\n",
        "\n",
        "Example 4:\n",
        "Review: \"Great food but the wait time was too long. Staff was friendly though.\"\n",
        "Response: {{\"predicted_stars\": 4, \"explanation\": \"Positive overall with good food and friendly staff, but one negative aspect (wait time)\"}}\n",
        "\n",
        "Example 5:\n",
        "Review: \"The worst restaurant I've ever been to. Food poisoning and rude staff.\"\n",
        "Response: {{\"predicted_stars\": 1, \"explanation\": \"Extremely negative with serious health and service issues\"}}\n",
        "\n",
        "Now classify this review:\n",
        "Review: {review_text}\n",
        "\n",
        "Return your response as a JSON object:\n",
        "{{\n",
        "    \"predicted_stars\": <number between 1 and 5>,\n",
        "    \"explanation\": \"<brief reasoning for the assigned rating>\"\n",
        "}}\n",
        "\n",
        "JSON Response:\"\"\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions for LLM Calls and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_llm(prompt: str, max_retries: int = 3) -> Dict:\n",
        "    \"\"\"Call LLM with retry logic\"\"\"\n",
        "    if model is None:\n",
        "        return {\"error\": \"Model not initialized\"}\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt)\n",
        "            text = response.text.strip()\n",
        "            \n",
        "            # Try to extract JSON from response\n",
        "            # Remove markdown code blocks if present\n",
        "            if '```json' in text:\n",
        "                text = text.split('```json')[1].split('```')[0].strip()\n",
        "            elif '```' in text:\n",
        "                text = text.split('```')[1].split('```')[0].strip()\n",
        "            \n",
        "            # Parse JSON\n",
        "            result = json.loads(text)\n",
        "            return result\n",
        "        except json.JSONDecodeError as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "            return {\"error\": f\"JSON decode error: {str(e)}\", \"raw_response\": text}\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(1)\n",
        "                continue\n",
        "            return {\"error\": str(e)}\n",
        "    \n",
        "    return {\"error\": \"Max retries exceeded\"}\n",
        "\n",
        "\n",
        "def evaluate_approach(df: pd.DataFrame, prompt_func, approach_name: str) -> Dict:\n",
        "    \"\"\"Evaluate a prompting approach on the dataset\"\"\"\n",
        "    results = []\n",
        "    valid_json_count = 0\n",
        "    \n",
        "    print(f\"\\nEvaluating {approach_name}...\")\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        review_text = str(row['text'])\n",
        "        actual_stars = int(row['stars'])\n",
        "        \n",
        "        prompt = prompt_func(review_text)\n",
        "        response = call_llm(prompt)\n",
        "        \n",
        "        if 'error' not in response and 'predicted_stars' in response:\n",
        "            valid_json_count += 1\n",
        "            predicted_stars = int(response['predicted_stars'])\n",
        "            explanation = response.get('explanation', '')\n",
        "            \n",
        "            results.append({\n",
        "                'actual': actual_stars,\n",
        "                'predicted': predicted_stars,\n",
        "                'correct': actual_stars == predicted_stars,\n",
        "                'explanation': explanation\n",
        "            })\n",
        "        else:\n",
        "            results.append({\n",
        "                'actual': actual_stars,\n",
        "                'predicted': None,\n",
        "                'correct': False,\n",
        "                'error': response.get('error', 'Unknown error')\n",
        "            })\n",
        "        \n",
        "        # Progress indicator\n",
        "        if (idx + 1) % 20 == 0:\n",
        "            print(f\"  Processed {idx + 1}/{len(df)} reviews...\")\n",
        "        \n",
        "        # Rate limiting\n",
        "        time.sleep(0.5)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    valid_results = [r for r in results if r['predicted'] is not None]\n",
        "    \n",
        "    if len(valid_results) == 0:\n",
        "        return {\n",
        "            'approach': approach_name,\n",
        "            'accuracy': 0,\n",
        "            'json_validity_rate': 0,\n",
        "            'total_reviews': len(df),\n",
        "            'valid_responses': 0\n",
        "        }\n",
        "    \n",
        "    correct_count = sum(1 for r in valid_results if r['correct'])\n",
        "    accuracy = correct_count / len(valid_results)\n",
        "    json_validity_rate = valid_json_count / len(df)\n",
        "    \n",
        "    # Calculate consistency (variance in predictions for same actual rating)\n",
        "    actual_to_predicted = {}\n",
        "    for r in valid_results:\n",
        "        actual = r['actual']\n",
        "        if actual not in actual_to_predicted:\n",
        "            actual_to_predicted[actual] = []\n",
        "        actual_to_predicted[actual].append(r['predicted'])\n",
        "    \n",
        "    consistency_scores = []\n",
        "    for actual, predictions in actual_to_predicted.items():\n",
        "        if len(predictions) > 1:\n",
        "            variance = np.var(predictions)\n",
        "            consistency_scores.append(1 / (1 + variance))  # Lower variance = higher consistency\n",
        "    \n",
        "    avg_consistency = np.mean(consistency_scores) if consistency_scores else 0\n",
        "    \n",
        "    return {\n",
        "        'approach': approach_name,\n",
        "        'accuracy': accuracy,\n",
        "        'json_validity_rate': json_validity_rate,\n",
        "        'consistency': avg_consistency,\n",
        "        'total_reviews': len(df),\n",
        "        'valid_responses': len(valid_results),\n",
        "        'correct_predictions': correct_count,\n",
        "        'results': results\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all three approaches\n",
        "results_approach_1 = evaluate_approach(df_sample, prompt_approach_1, \"Approach 1: Direct Classification\")\n",
        "results_approach_2 = evaluate_approach(df_sample, prompt_approach_2, \"Approach 2: Chain-of-Thought\")\n",
        "results_approach_3 = evaluate_approach(df_sample, prompt_approach_3, \"Approach 3: Few-Shot Learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_data = {\n",
        "    'Approach': [\n",
        "        results_approach_1['approach'],\n",
        "        results_approach_2['approach'],\n",
        "        results_approach_3['approach']\n",
        "    ],\n",
        "    'Accuracy': [\n",
        "        f\"{results_approach_1['accuracy']:.3f}\",\n",
        "        f\"{results_approach_2['accuracy']:.3f}\",\n",
        "        f\"{results_approach_3['accuracy']:.3f}\"\n",
        "    ],\n",
        "    'JSON Validity Rate': [\n",
        "        f\"{results_approach_1['json_validity_rate']:.3f}\",\n",
        "        f\"{results_approach_2['json_validity_rate']:.3f}\",\n",
        "        f\"{results_approach_3['json_validity_rate']:.3f}\"\n",
        "    ],\n",
        "    'Consistency': [\n",
        "        f\"{results_approach_1.get('consistency', 0):.3f}\",\n",
        "        f\"{results_approach_2.get('consistency', 0):.3f}\",\n",
        "        f\"{results_approach_3.get('consistency', 0):.3f}\"\n",
        "    ],\n",
        "    'Valid Responses': [\n",
        "        results_approach_1['valid_responses'],\n",
        "        results_approach_2['valid_responses'],\n",
        "        results_approach_3['valid_responses']\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON TABLE\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrices for each approach\n",
        "def create_confusion_matrix(results_dict, approach_name):\n",
        "    results = results_dict['results']\n",
        "    valid_results = [r for r in results if r['predicted'] is not None]\n",
        "    \n",
        "    if len(valid_results) == 0:\n",
        "        print(f\"\\n{approach_name}: No valid results\")\n",
        "        return\n",
        "    \n",
        "    confusion = np.zeros((5, 5), dtype=int)\n",
        "    for r in valid_results:\n",
        "        actual_idx = int(r['actual']) - 1\n",
        "        predicted_idx = int(r['predicted']) - 1\n",
        "        confusion[actual_idx, predicted_idx] += 1\n",
        "    \n",
        "    print(f\"\\n{approach_name} - Confusion Matrix:\")\n",
        "    print(\"\\nActual \\\\ Predicted\", end=\"\")\n",
        "    for i in range(1, 6):\n",
        "        print(f\"\\t{i}\", end=\"\")\n",
        "    print()\n",
        "    \n",
        "    for i in range(5):\n",
        "        print(f\"{i+1}\", end=\"\")\n",
        "        for j in range(5):\n",
        "            print(f\"\\t{confusion[i, j]}\", end=\"\")\n",
        "        print()\n",
        "\n",
        "create_confusion_matrix(results_approach_1, \"Approach 1\")\n",
        "create_confusion_matrix(results_approach_2, \"Approach 2\")\n",
        "create_confusion_matrix(results_approach_3, \"Approach 3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion and Trade-offs\n",
        "\n",
        "### Approach 1: Direct Classification\n",
        "- **Pros**: Simple, fast, low token usage\n",
        "- **Cons**: May miss nuanced sentiment, less reasoning transparency\n",
        "\n",
        "### Approach 2: Chain-of-Thought\n",
        "- **Pros**: More transparent reasoning, better for complex reviews\n",
        "- **Cons**: Higher token usage, slower processing\n",
        "\n",
        "### Approach 3: Few-Shot Learning\n",
        "- **Pros**: Learns from examples, potentially better pattern recognition\n",
        "- **Cons**: Requires good example selection, higher token usage\n",
        "\n",
        "### Key Findings:\n",
        "1. **Accuracy**: [To be filled after evaluation]\n",
        "2. **JSON Validity**: [To be filled after evaluation]\n",
        "3. **Consistency**: [To be filled after evaluation]\n",
        "\n",
        "### Recommendations:\n",
        "Based on the evaluation results, [recommendation will be added after running the evaluation]"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
